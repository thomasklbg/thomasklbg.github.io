---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I'm a Postdoctoral Research Associate at [The Alan Turing Institute](https://www.turing.ac.uk/){:target="_blank"} 
and an Associate Member of the Department of Computer Science at the [University of Oxford](https://www.cs.ox.ac.uk/){:target="_blank"}, where I'm mentored by [Marta Kwiatkowska](https://www.trinity.ox.ac.uk/people/marta-kwiatkowska){:target="_blank"} and 
[Lukasz Szpruch](https://scholar.google.com/citations?hl=en&user=ljeA6CMAAAAJ&view_op=list_works&sortby=pubdate){:target="_blank"}. 
From 2021 to 2024, I completed my PhD at the [University of Oslo](https://www.mn.uio.no/ifi/english/){:target="_blank"} supervised by [Christos Dimitrakakis](https://sites.google.com/site/christosdimitrakakis){:target="_blank"}, and previously studied Mathematics (BSc, MSc).      


My research interests are in reinforcement learning and related areas, including reward learning, preference learning, and the intersection of machine learning with game theory and mechanism design.



## News 
- **04/2025**: The 2nd Edition of last year's workshop on [Models of Human Feedback for AI Alignment](https://sites.google.com/view/mhf-icml2025){:target="_blank"} is taking place on July 18th at ICML 2025! [Submission Deadline is May 25th!](https://sites.google.com/view/mhf-icml2025/call-for-papers)
- **03/2025**: We released three new preprints on [Strategyproof RLHF](https://arxiv.org/pdf/2503.09561){:target="_blank"}, [Causal Imitation Learning](https://arxiv.org/pdf/2502.07656){:target="_blank"} and [Multi-Agent Cooperative RL](https://arxiv.org/pdf/2502.02377){:target="_blank"}.  
- **11/2024**: I'm visiting Haifeng Xu's group at the University of Chicago. I'll give a talk on [Strategic Interactive Decision-Making](https://cs.uchicago.edu/events/event/thomas-kleine-buening-oxford-strategic-interactive-decision-making/) on December 5th at the CS Department.  
- **07/2024**: We're organizing the ICML 2024 Workshop on [Models of Human Feedback for AI Alignment](https://sites.google.com/view/mhf-icml2024){:target="_blank"}. 09/2024: Recordings are now available [here](https://icml.cc/virtual/2024/workshop/29943){:target="_blank"}.


## Selected Publications  

* **Strategyproof Reinforcement Learning from Human Feedback** [[pdf](https://arxiv.org/pdf/2503.09561){:target="_blank"}] <br />
Thomas Kleine Buening, Jiarui Gan, Debmalya Mandal, Marta Kwiatkowska <br />
working paper  


* **Strategic Linear Contextual Bandits** [[pdf](https://arxiv.org/pdf/2406.00551){:target="_blank"}] <br />
Thomas Kleine Buening, Aadirupa Saha, Christos Dimitrakakis, Haifeng Xu <br />
NeurIPS 2024


* **Environment Design for Inverse Reinforcement Learning** [[pdf](https://arxiv.org/pdf/2210.14972v3){:target="_blank"}] <br /> 
Thomas Kleine Buening$^\star$, Victor Villin$^\star$, Christos Dimitrakakis <br /> 
ICML 2024, <span style="color:red">Oral Presentation</span>




You can reach me at tbuening@turing.ac.uk or thomas.kleinebuening@cs.ox.ac.uk.

